# -*- coding: utf-8 -*-
"""HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mt_ie6gCxJPQFlZxfk6KwgG_5IdZuduL

Q2
"""

import pandas as pd
import numpy as np
from sklearn.datasets import load_boston
boston_dataset = load_boston()
boston = pd.DataFrame(boston_dataset.data,columns=boston_dataset.feature_names)
boston['MEDV'] = boston_dataset.target
boston.head()

from sklearn import datasets, linear_model 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
X = boston_dataset.data
y = boston_dataset.target
def findScore(X, y):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
  clf = Ridge(alpha=1.0)
  model = clf.fit(X_train, y_train)
  score = model.score(X_test, y_test)
  print("Score:", score)
  return score

for i in range(0,10):
  findScore(X,y)

"""Q4"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_blobs
center = [[-0.5, -0.5],[0.5, 0.5]]
X_orin, y_orin = make_blobs(n_samples=200, centers=center, cluster_std=0.3, random_state=0)
plt.title("Two Clusters of Data")
plt.scatter(X_orin[:,0], X_orin[:,1])

steps = 200000
learningrate = 0.001
X, y = X_orin.T, y_orin.reshape(1, y_orin.shape[0])
w = np.random.randn(X.shape[0], 1)*0.01
import math, time

GD_losstrack = []
# Time for one iteration of updating w
tall = 0
for step in range(steps):
  t_start = time.time()
  p = 1/(1+np.exp(-np.dot(w.T, X)))
  Lw = -np.sum(np.multiply(np.log(p), y) + np.multiply((1 - y), np.log(1 - p)))
  GD_losstrack.append(np.squeeze(Lw))
  dz = p-y
  dw = np.dot(X, dz.T)
  w = w - learningrate * dw
  t_end = time.time()
  tall += t_end-t_start
print("GD running time: ",tall)
print("L(w) by GD:",Lw)
plt.plot(GD_losstrack,color='r',label = 'GD')

import random
steps = 200000
wsgd = np.zeros(2)
batch = 10
learningrate = 0.001
ws = np.random.randn(X.shape[0], 1)*0.01

def SGD(batch, learningrate):
  ws = np.random.randn(X.shape[0], 1)*0.01
  t_start = time.time()
  SGD_losstrack = []
  total = 0
  for step in range(steps):
    start = random.randint(0,200-batch)
    X_sample = [X[0][start:start+10],X[1][start:start+10]]
    y_sample = [y[0][start:start+10]]
    # Time for each iteration of updating w
    t_start = time.time()
    pi = 1/(1+np.exp(-np.dot(ws.T, X_sample)))
    dws = np.dot(X_sample, (pi-y_sample).T)
    ws = ws - learningrate*dws
    t_end = time.time()
    total += t_end-t_start
    p = 1/(1+np.exp(-np.dot(ws.T, X)))
    Lw = -np.sum(np.multiply(np.log(p), y) + np.multiply((1 - y), np.log(1 - p)))
    SGD_losstrack.append(np.squeeze(Lw))
  print ("SGD running time: ",total)
  return [SGD_losstrack,ws]

SGD_res = SGD(1,0.001)
print("L(w) by SGD:",SGD_res[0][-1])
BGD_res = SGD(20,0.001)
print("L(w) by BatchSGD:",BGD_res[0][-1])

fig, ax = plt.subplots(dpi=100)
ax.set_xlabel('Iterations')
ax.set_ylabel('Loss Function Value')
ax.plot(GD_losstrack,color='r',label = 'GD')
ax.plot(SGD_res[0],color='y',label = 'SGD')
plt.ylim(5,70)
ax.plot(BGD_res[0],color='blue',label = 'MiniBatchGD')
plt.legend(loc=0)
plt.show()

"""In this section, I tried out two models. One is SGD with batchsize=1, and the other one is minibatch GD with batchsize=10. The curves of these two models become similar when learning rate is small and number of iterations is large.
Regardless of random function and other items, focusing on calculating w, the running time of GD is longer than that of SGD, because it will need more time for each iteration.
According to the graph above, SGD and minibatch GD have slow rates of convergence than full batch GD.
In addition, as the iteration number increasing, result of SGD will finally approach the result of GD.
"""

# Plot the results from GD(Red), SGD(Yellow)
X_ax = X_orin[:,0]
y1_ax = -w[0]*X_orin[:,0]/w[1]
y2_ax = -SGD_res[0][0]*X_orin[:,0]/SGD_res[0][1]

fig = plt.figure(figsize=(5,5))
ax = fig.add_subplot(1,1,1)
ax.scatter(X_orin[:,0], X_orin[:,1])
ax.plot(X_ax, y1_ax, color='r') # GD
ax.plot(X_ax, y2_ax, color='y') # SGD

"""According to this graph, the models worked out by GD and SGD are both fitable for the classification."""
