# -*- coding: utf-8 -*-
"""HW4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j_CwWDnfkFcn4y1GPQWi1_XlhcQbpM1Y

Q4. KNN and SVM using tensorflow
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

train_images = np.reshape(train_images,(60000,784))
test_images = np.reshape(test_images,(10000,784))
print(train_images.shape)
print(test_images.shape)

plt.figure(figsize=(20,20))
def find10(c):
  l = [[],[]]
  count = 0
  for i in range(0,1000):
    if train_labels[i] == c and count < 10:
      l[0].append(train_images[i])
      l[1].append(train_labels[i])
      count += 1
  return l
a = []
b = []
for j in range(0,10):
  m = find10(j)
  for k in range(0,10):
    a.append(m[0][k])
    b.append(m[1][k])

for i in range(100):
  plt.subplot(10,10,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(np.reshape(a[i],(28,28)))
  plt.xlabel(b[i])
plt.show()

# Commented out IPython magic to ensure Python compatibility.
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

logisticReg = LogisticRegression(penalty='l2',tol=0.1,solver='saga',C=2)
# %time logisticReg.fit(train_images,train_labels)

# %time logistic_score = logisticReg.score(test_images,test_labels)
print(logistic_score)

# Commented out IPython magic to ensure Python compatibility.
from sklearn.neighbors import KNeighborsClassifier
train_images = train_images[0:10000]
train_labels = train_labels[0:10000]
test_images = test_images[0:1000]
test_labels = test_labels[0:1000]
neigh = KNeighborsClassifier(n_neighbors=3)
# %time neigh.fit(train_images, train_labels)
# %time knn_score = neigh.score(test_images,test_labels)
print(knn_score)

# Commented out IPython magic to ensure Python compatibility.
from sklearn import svm
svc = svm.SVC(probability=False,kernel="linear",C=5,max_iter=1000,gamma=0.01)
# %time svc.fit(train_images,train_labels)
# %time svc_score = svc.score(test_images,test_labels)
print(svc_score)

svcrbf = svm.SVC(probability=False,kernel="rbf",C=5,max_iter=1000,
                 decision_function_shape='ovo',gamma='scale')
# %time svcrbf.fit(train_images,train_labels)
# %time svcrbf_score = svcrbf.score(test_images,test_labels)
print(svcrbf_score)

"""Q2. Perception algorithm on synthetic data"""

import numpy as np
from sklearn import neighbors, datasets
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt 
from matplotlib.colors import ListedColormap
cmap_bold = ListedColormap(['darkorange', 'c'])

center = [[-0.5, -0.5],[0.5, 0.5]]
X, Y = make_blobs(n_samples=200, centers=center, cluster_std=0.8, random_state=0)
plt.title("Two Clusters of Data")
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=cmap_bold, edgecolor='k', s=100)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)

#initialize Weights to zero values
W = np.zeros((2,1))
# set some learning rate for gradient descent
lr = 1

for epoch in range(100):
    for idx in range(len(X_train)):

        # update weights if the prediction is wrong
        
        if Y_train[idx]*X_train[idx,:].reshape(1,-1)@W <= 0:
            # update weights if the point is incorrectly classified
            # W(t+1) = W(t) + lr * y_i*x_i
            W = W + (lr)*Y_train[idx]*X_train[idx,:].reshape(-1,1)
    
    if epoch % 5 == 0:
        # print accruacy on test data
        # keep a track of everything happeniing here.. . . .
        Y_test_pred = (X_test@W).reshape(-1)
        Y_test_pred[Y_test_pred < 0] = -1
        Y_test_pred[Y_test_pred >= 0] = 1
        
        acc = np.mean(Y_test == Y_test_pred)
        print(f"Epoch: {epoch}, Test Accuracy: {acc}")

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
h = 0.1
cmap_light = ListedColormap(['orange', 'cyan'])
cmap_bold = ListedColormap(['darkorange', 'c'])

x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1
y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

Z = np.c_[xx.ravel(), yy.ravel()]
Z = Z@W
Z[Z<0] = -1
Z[Z>=0] = 1

# Put the result into a color plot
Z = Z.reshape(xx.shape)

plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
    
# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=cmap_bold, edgecolor='k', s=100)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())

plt.show()

"""If the two clusters are far away from each other(std=0.1), the implementation will converge very fast at the beginning, within 5 epochs. 
If the two clusters overlap(std=0.8), it will not converge, but oscillate around, and the classification regions derived are not the truth.
"""