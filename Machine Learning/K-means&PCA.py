# -*- coding: utf-8 -*-
"""ML_HW6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/110Ongp9rdy57bSk5nYZxvK0D2d15fxS3

Q1. A simple PCA analysis
"""

import numpy as np 
sample = np.array([[3,2,1],[2,4,5],[1,2,3],[0,2,5]])
mean = np.mean(sample, axis=0)
print("Sample is: \n", sample)
print("Mean is: ", mean)

csample = sample - mean
print("Zero-centered sample: \n", csample)
q = np.cov(csample.T)
print("Covariance matrix: \n", q)

from numpy import linalg
eigen = np.linalg.eig(q)
print("Eigenvalues: ", eigen[0])
print("Eigenvectors: \n", eigen[1])

from sklearn.decomposition import PCA

pca = PCA(n_components=1)
pca.fit(csample)
a_1 = pca.components_.T # get a_1 that maximizes variance
print("Components:\n", a_1)

# we compute scores along first principal component with the top eigenvector
scores_a_1 = np.matmul(csample, a_1) 
print("Coefficients:\n", scores_a_1)

"""The signs of all the values in components computed by PCA are opposite to those of eigenvectors calculated by np.linalg.eig().

> From online searching results, this difference has no influence for eigenvectors. Therefore, the results from PCA package is chosen in the this answer.
"""

pca = PCA(n_components=2)
projected = pca.fit_transform(csample)
reconstruct = projected.dot(pca.components_)
print("Reconstruction matrix is:\n", reconstruct)

error = []
for i in range(len(csample)):
  error.append(np.linalg.norm(csample[i]-reconstruct[i]))
print("Reconstruction errors for each sample are:\n", error)

"""Q3. Use Senate data to do K-means analysis"""

import pandas as pd

url_votes = 'https://raw.githubusercontent.com/exemplary-citizen/PCA-and-Senate-Voting-Data/master/senator_pca_problem/senator_data_pca/'
senator_df =  pd.read_csv(url_votes + 'data_matrix.csv',error_bad_lines=False)
af = pd.read_csv(url_votes + 'politician_labels.txt', header=None)
af["affiliations"] = af[0].str.split().str[-1]

X = np.array(senator_df.values[:, 3:].T, dtype='float64') 

typical_row = X[0,:]
print(typical_row.shape)
print(typical_row)
print(af.shape)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

X_original = X.copy()
X = X - np.mean(X, axis = 0)
affiliations = af["affiliations"]
print(affiliations)

a_rand = np.random.rand(542,1) #generate a random direction
a_rand = a_rand/np.linalg.norm(a_rand) #we normalize the vector
scores_rand = np.matmul(X, a_rand)

plt.scatter(scores_rand, np.zeros_like(scores_rand), c=affiliations)
plt.title('Projections along random direction')
plt.show()

from sklearn.cluster import KMeans
kmeans_model = KMeans(n_clusters=2, random_state=1).fit(X)

labels = kmeans_model.predict(X)

samples = projected

pca = PCA(n_components=2)
projected = pca.fit_transform(X) 

plt.scatter(projected[:, 0], projected[:, 1], c=affiliations)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Projection on two principal components')
plt.show()

plt.scatter(samples[:, 0], samples[:, 1], c=labels, s=40, cmap='viridis')
ax = plt.gca()
ax.axis('equal')
plt.show()

print(X.shape)
print(labels.shape)

"""> Comparing the two graphs above, the first one is the results got from k-means, and the second one is from demo 11. 



> There are four points wrong: one point belong to blue(label==0) labeled red(label==1), one point belong to red labeled blue, and four points belong to yellow labeled blue.
"""

colors = []
for l in labels:
  if l == 0:
    colors.append("Blue")
  else:
    colors.append("Red")
print("People with wrong labels are:")
for i in range(100):
  if affiliations[i] != colors[i]:
    print(i, af[0][i])

"""Q4. PCA analysis to find outlier cities using palces.txt"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

pfile = open("places.txt","r+")
attri = pfile.readline().split()
cities = []
print(attri)  
inputs = []
while(True):
  line = pfile.readline()
  if line == "":
    break
  lst = line.split()
  cities.append(lst[0])
  l = []
  for i in range(1,10):
    l.append(int(lst[i]))
  inputs.append(l)
 
X = np.array(inputs) 
print(X)
print(X.shape)

X_ori = X.copy()

X = np.log10(X)
print(X)

X = X - np.mean(X, axis = 0)
print(X)

pca = PCA(n_components=9, svd_solver='randomized', whiten=True)
pca.fit(X)
compos = pca.components_
print(compos)

v1 = compos[0]
v2 = compos[1]
print("V1:\n", v1)
a_v1 = np.absolute(v1)
max1 = np.where(a_v1==np.amax(a_v1))[0][0]
print("Most correlated in V1 is: ", attri[max1])
print("V2:\n", v2)
a_v2 = np.absolute(v2)
max2 = np.where(a_v2==np.amax(a_v2))[0][0]
print("Most correlated in V2 is: ", attri[max2])

proj = pca.fit_transform(X)

scores = np.matmul(X, proj.T) 

two_scores = []

for i in scores:
  a_i = np.absolute(i)
  indexes = a_i.argsort()[-2:][::-1]
  two_scores.append([a_i[indexes[0]], a_i[indexes[1]]])
two_scores = np.array(two_scores)

plt.figure(figsize=(12,8))
plt.scatter(two_scores[:, 0], two_scores[:, 1])
for i in range(len(cities)):
  plt.annotate(i,(two_scores[:, 0][i], two_scores[:, 1][i]))
plt.title('Distribution of highest two scores of cities')
plt.show()

out_index = [212,278]
outlier = []
for i in out_index:
  outlier.append(cities[i])
print("Outlier cities are: ", outlier)

# Use z-score matrix

from scipy import stats
X_z = stats.zscore(X_ori)
#X_z = X_z - np.mean(X_z, axis = 0)
print(X_z)

pca = PCA(n_components=9, svd_solver='randomized', whiten=True)
pca.fit(X_z)
compos = pca.components_

v1 = compos[0]
v2 = compos[1]
print("V1:\n", v1)
a_v1 = np.absolute(v1)
max1 = np.where(a_v1==np.amax(a_v1))[0][0]
print("Most correlated in V1 is: ", attri[max1])
print("V2:\n", v2)
a_v2 = np.absolute(v2)
max2 = np.where(a_v2==np.amax(a_v2))[0][0]
print("Most correlated in V2 is: ", attri[max2])

proj = pca.fit_transform(X_z)

scores = np.matmul(X_z, proj.T) 

two_scores = []

for i in scores:
  a_i = np.absolute(i)
  indexes = a_i.argsort()[-2:][::-1]
  two_scores.append([a_i[indexes[0]], a_i[indexes[1]]])
two_scores = np.array(two_scores)

plt.figure(figsize=(12,8))
plt.scatter(two_scores[:, 0], two_scores[:, 1])
for i in range(len(cities)):
  plt.annotate(i,(two_scores[:, 0][i], two_scores[:, 1][i]))
plt.title('Distribution of highest two scores of cities')
plt.show()

out_index = [212,269,191,178]
outlier = []
for i in out_index:
  outlier.append(cities[i])
print("Outlier cities are: ", outlier)

"""> The most correlated feature for V2 is changed.

> The cluster of cities is more compressed than before and the outlier cities are more separated from the cluster, so there are more cities able to be distinguished.
"""